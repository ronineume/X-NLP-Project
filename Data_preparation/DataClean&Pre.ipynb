{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d745c71b-28bc-4894-bd71-d4ff21750c70",
   "metadata": {},
   "source": [
    "# Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f420d2-c50e-4436-8836-420c4f938428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('output_tweets.csv')\n",
    "\n",
    "# Step 1: Change 'created_at' column to a format that Excel can recognize\n",
    "# Convert to datetime format (removing 'T' and 'Z' from the ISO timestamp)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "# Step 2: Modify 'edit_history_tweet_ids' column\n",
    "# If there is only one code inside [' '], change to 0, otherwise change to 1\n",
    "df['edit_history_tweet_ids'] = df['edit_history_tweet_ids'].apply(lambda x: 0 if len(eval(x)) == 1 else 1)\n",
    "\n",
    "# Step 3: Modify 'referenced_tweet_types' column\n",
    "# Convert based on the conditions provided\n",
    "def convert_referenced_tweet_types(val):\n",
    "    val = eval(val)  # Convert the string representation of the list to a list\n",
    "    if not val:  # Empty list\n",
    "        return 0\n",
    "    elif val == ['replied_to']:\n",
    "        return 1\n",
    "    elif val == ['quoted']:\n",
    "        return 2\n",
    "    elif set(val) == {'quoted', 'replied_to'}:\n",
    "        return 3\n",
    "    return 0  # Default case, if unmatched\n",
    "\n",
    "df['referenced_tweet_types'] = df['referenced_tweet_types'].apply(convert_referenced_tweet_types)\n",
    "\n",
    "# Step 4: Remove rows where the 'text' column contains more than 5 hashtags (#)\n",
    "df = df[df['text'].apply(lambda x: x.count('#') <= 5)]\n",
    "\n",
    "# Step 4.5: Remove rows where the 'text' column contains the specific phrase\n",
    "specific_text = \"Tesla stocks are skyrocketing ðŸš€ðŸ“ˆ!\"\n",
    "#Tesla stocks are skyrocketing ðŸš€ðŸ“ˆ! Investors are already seeing big gains, what are you waiting for? Time to jump in and ride the wave!\n",
    "df = df[~df['text'].str.contains(re.escape(specific_text), case=False, na=False)]\n",
    "specific_text = \"memecoin\"\n",
    "df = df[~df['text'].str.contains(re.escape(specific_text), case=False, na=False)]\n",
    "specific_text = \"bitcoin\"\n",
    "df = df[~df['text'].str.contains(re.escape(specific_text), case=False, na=False)]\n",
    "specific_text = \"Buy https\"\n",
    "df = df[~df['text'].str.contains(re.escape(specific_text), case=False, na=False)]\n",
    "specific_text = \"To place your order\"\n",
    "df = df[~df['text'].str.contains(re.escape(specific_text), case=False, na=False)]\n",
    "# Step 5: Save the modified dataframe to a new CSV file\n",
    "df.to_csv('processed_output_tweets.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"CSV processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f70be-fac5-4606-90e7-069f983a04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('output_users.csv')\n",
    "\n",
    "# Convert the 'created_at' column to a more Excel-friendly datetime format\n",
    "df['created_at'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Convert the 'verified' column: True -> 1, False -> 0\n",
    "df['verified'] = df['verified'].apply(lambda x: 1 if x == True else 0)\n",
    "df = df.drop_duplicates(subset='id')\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('processed_output_users.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c22c7-2ce1-4c7e-8651-018cd4a2b808",
   "metadata": {},
   "source": [
    "## Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f75b11-c621-4805-8ef8-31e76d2de8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV files\n",
    "tweets_df = pd.read_csv('processed_output_tweets.csv')\n",
    "users_df = pd.read_csv('processed_output_users.csv')\n",
    "users_df = users_df.drop_duplicates(subset='id')\n",
    "# Print the size of the original DataFrames\n",
    "print(f\"Tweets DataFrame Size: {tweets_df.shape}\")\n",
    "print(f\"Users DataFrame Size: {users_df.shape}\")\n",
    "\n",
    "# Perform a left join to merge the two DataFrames using author_id and id\n",
    "merged_df = pd.merge(tweets_df, users_df, left_on='author_id', right_on='id', how='left')\n",
    "merged_df = merged_df.drop(columns=['id_y'])\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "# Print the size of the merged DataFrame\n",
    "print(f\"Merged DataFrame Size: {merged_df.shape}\")\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file with utf-8-sig encoding\n",
    "merged_df.to_csv('Cleaned_merged_data.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4bf58e-dbec-401f-b789-0f3eb8cc25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('Cleaned_merged_data.csv')\n",
    "text = df['text'].iloc[1:].astype(str)\n",
    "\n",
    "# Print type and content for debugging\n",
    "print(f\"Data type: {type(text)}\")\n",
    "print(text.head())\n",
    "\n",
    "# Define function for text preprocessing\n",
    "def preprocess_text(t):\n",
    "    # Replace newline characters with a space to ensure each tweet occupies one line\n",
    "    \n",
    "    # Replace HTML entities\n",
    "    t = t.replace(\"&nbsp;\", \" \")\n",
    "    t = t.replace(\"&lt;\", \"<\")\n",
    "    t = t.replace(\"&gt;\", \">\")\n",
    "    t = t.replace(\"&amp;\", \"&\")\n",
    "    t = t.replace(\"&quot;\", \"\")\n",
    "    t = t.replace(\"&apos;\", \"\")\n",
    "    t = t.replace(\"&times;\", \"Ã—\")\n",
    "    t = t.replace(\"&divide;\", \"Ã·\")\n",
    "\n",
    "    # Replace hashtags\n",
    "    t = re.sub(r'#(\\w+)', r'HASH_\\1', t)\n",
    "\n",
    "    # Replace handles\n",
    "    t = re.sub(r'@(\\w+)', r'HNDL_\\1', t)\n",
    "\n",
    "    # Replace URLs\n",
    "    t = re.sub(r'(http|https|ftp)://[a-zA-Z0-9\\./]+', 'URL', t)\n",
    "\n",
    "    # Replace emoticons\n",
    "    emoticon_dict = {\n",
    "        r'[:-]?\\)|:\\)|\\(:|\\(-:': 'EMOT_SMILEY',  # Smiley face\n",
    "        r'[:-]?D|:D|X-D|XD|xD': 'EMOT_LAUGH',   # Laughter\n",
    "        r'<3|:\\*': 'EMOT_LOVE',                # Heart\n",
    "        r';-?\\)|;\\)|;-?D|;D|\\(-;|;\\(': 'EMOT_WINK',  # Wink\n",
    "        r'[:-]?\\(|:\\(': 'EMOT_FROWN',          # Frown\n",
    "        r',\\(|:\\'|:\"|\\(\\(': 'EMOT_CRY'         # Crying\n",
    "    }\n",
    "    for emoticon, replacement in emoticon_dict.items():\n",
    "        t = re.sub(emoticon, replacement, t)\n",
    "\n",
    "    # Replace punctuation\n",
    "    t = re.sub(r'([!.Â¡Â¿])', r' PUNC_EXCL ', t)  # Replace exclamation marks\n",
    "    t = re.sub(r'\\.', ' PUNC_DOT ', t)          # Replace periods\n",
    "    t = re.sub(r'\\?', ' PUNC_QUES ', t)        # Replace question marks\n",
    "    t = re.sub(r'\\.\\.\\.|â€¦', ' PUNC_ELLP ', t)  # Replace ellipses\n",
    "\n",
    "    # Replace repeated characters\n",
    "    t = re.sub(r'(.)\\1{1,}', r'\\1\\1', t)\n",
    "\n",
    "    # Remove extra single quotes and other unnecessary characters\n",
    "    t = re.sub(r'[\\'â€œâ€]', '', t)\n",
    "    t = t.strip()\n",
    "\n",
    "    return t\n",
    "\n",
    "# Apply text preprocessing\n",
    "try:\n",
    "    processed_texts = df['text'].apply(preprocess_text)\n",
    "    df['text'] = processed_texts  # Update the text column in the original DataFrame\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during preprocessing: {e}\")\n",
    "    processed_texts = None  # Ensure the variable is defined\n",
    "\n",
    "# Check if processed_texts is successfully defined\n",
    "if processed_texts is not None:\n",
    "    # Save the results to a new CSV file\n",
    "    df.to_csv('processed_cleaned_data.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"Processing completed, results saved to 'processed_cleaned_data1.csv'\")\n",
    "else:\n",
    "    print(\"Processing failed, no results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69864a55-2a3c-46c7-8100-fd0e543dd022",
   "metadata": {},
   "source": [
    "# Classification (Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0867d-29dc-4208-a130-0dab03ae255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from collections import defaultdict\n",
    "\n",
    "# Give Input/Output file paths\n",
    "input_file = \"processed_cleaned_data.csv\"  # Provide your own\n",
    "\n",
    "# Set confidence_threshold\n",
    "confidence_threshold = 0.9  # In our setting, the default is 0.9\n",
    "\n",
    "tweets = pd.read_csv(\"processed_cleaned_data.csv\")\n",
    "tweets = tweets['text']\n",
    "# Store trusted tweets\n",
    "trusted_tweets = []\n",
    "# Dictionary for counting languages\n",
    "language_count = defaultdict(int)\n",
    "\n",
    "# Initialize LanguageIdentifier\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "# Iterate over each tweet\n",
    "for tweet in tweets:\n",
    "    text = tweet  # Already a string, no need to call get\n",
    "    \n",
    "    # Use langid.py for language identification\n",
    "    lang, confidence = identifier.classify(text)\n",
    "    # Count languages\n",
    "    language_count[lang] += 1\n",
    "    \n",
    "    # Check if the language is English and confidence is above the threshold\n",
    "    if lang == 'en' and confidence >= confidence_threshold:\n",
    "        trusted_tweets.append({'text': text, 'language': lang})  # Store tweet and language information\n",
    "\n",
    "# Print counts of different languages\n",
    "print(\"Language counts:\")\n",
    "for lang, count in language_count.items():\n",
    "    print(f\"{lang}: {count}\")\n",
    "print(f\"We got {len(trusted_tweets)} trusted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce351cd-2f35-49de-9e8f-b7baac8f5a9a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477c6f5-35c1-4575-9fef-fa0ba1cd8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(\"processed_cleaned_data.csv\")\n",
    "\n",
    "# Define a function for tokenization\n",
    "def tokenize_text(text):\n",
    "    # Use TextBlob for tokenization\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    return blob.words  # Return the list of tokens\n",
    "\n",
    "# Apply the function to each row of text and create a new column 'token'\n",
    "df['token'] = df['text'].apply(tokenize_text)\n",
    "\n",
    "# Print the first few rows to confirm\n",
    "print(df[['text', 'token']].head())\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_file = \"tokenized_data.csv\"\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Tokenization completed and saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d9b07-a6c5-400a-8939-cdcd1c03f11b",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe0842-3ff4-48b9-9ce4-b3a9aad99fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Paths for input and output files\n",
    "input_file = \"tokenized_data.csv\"  # Provide your own\n",
    "output_file = \"normalized_data.csv\"\n",
    "\n",
    "# Create Normalization Lexicon\n",
    "normalization_lexicon = {\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"b4\": \"before\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"pls\": \"please\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"shuld\": \"should\",\n",
    "    \"4\": \"for\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"sucha\": \"such a\",\n",
    "    \"ainâ€™t\": \"are not\",\n",
    "    \"â€™s\": \"is\",\n",
    "    \"arenâ€™t\": \"are not\",\n",
    "    \"canâ€™t\": \"cannot\",\n",
    "    \"canâ€™tâ€™ve\": \"cannot have\",\n",
    "    \"â€™cause\": \"because\",\n",
    "    \"couldâ€™ve\": \"could have\",\n",
    "    \"couldnâ€™t\": \"could not\",\n",
    "    \"couldnâ€™tâ€™ve\": \"could not have\",\n",
    "    \"didnâ€™t\": \"did not\",\n",
    "    \"doesnâ€™t\": \"does not\",\n",
    "    \"donâ€™t\": \"do not\",\n",
    "    \"hadnâ€™t\": \"had not\",\n",
    "    \"hadnâ€™tâ€™ve\": \"had not have\",\n",
    "    \"hasnâ€™t\": \"has not\",\n",
    "    \"havenâ€™t\": \"have not\",\n",
    "    \"heâ€™d\": \"he would\",\n",
    "    \"heâ€™dâ€™ve\": \"he would have\",\n",
    "    \"heâ€™ll\": \"he will\",\n",
    "    \"heâ€™llâ€™ve\": \"he will have\",\n",
    "    \"howâ€™d\": \"how did\",\n",
    "    \"howâ€™dâ€™y\": \"how do you\",\n",
    "    \"howâ€™ll\": \"how will\",\n",
    "    \"Iâ€™d\": \"I would\",\n",
    "    \"Iâ€™dâ€™ve\": \"I would have\",\n",
    "    \"Iâ€™ll\": \"I will\",\n",
    "    \"Iâ€™llâ€™ve\": \"I will have\",\n",
    "    \"Iâ€™m\": \"I am\",\n",
    "    \"Iâ€™ve\": \"I have\",\n",
    "    \"isnâ€™t\": \"is not\",\n",
    "    \"itâ€™d\": \"it would\",\n",
    "    \"itâ€™dâ€™ve\": \"it would have\",\n",
    "    \"itâ€™ll\": \"it will\",\n",
    "    \"itâ€™llâ€™ve\": \"it will have\",\n",
    "    \"letâ€™s\": \"let us\",\n",
    "    \"maâ€™am\": \"madam\",\n",
    "    \"maynâ€™t\": \"may not\",\n",
    "    \"mightâ€™ve\": \"might have\",\n",
    "    \"mightnâ€™t\": \"might not\",\n",
    "    \"mightnâ€™tâ€™ve\": \"might not have\",\n",
    "    \"mustâ€™ve\": \"must have\",\n",
    "    \"mustnâ€™t\": \"must not\",\n",
    "    \"mustnâ€™tâ€™ve\": \"must not have\",\n",
    "    \"neednâ€™t\": \"need not\",\n",
    "    \"neednâ€™tâ€™ve\": \"need not have\",\n",
    "    \"oâ€™clock\": \"of the clock\",\n",
    "    \"oughtnâ€™t\": \"ought not\",\n",
    "    \"oughtnâ€™tâ€™ve\": \"ought not have\",\n",
    "    \"shanâ€™t\": \"shall not\",\n",
    "    \"shaâ€™nâ€™t\": \"shall not\",\n",
    "    \"shanâ€™tâ€™ve\": \"shall not have\",\n",
    "    \"sheâ€™d\": \"she would\",\n",
    "    \"sheâ€™dâ€™ve\": \"she would have\",\n",
    "    \"sheâ€™ll\": \"she will\",\n",
    "    \"sheâ€™llâ€™ve\": \"she will have\",\n",
    "    \"shouldâ€™ve\": \"should have\",\n",
    "    \"shouldnâ€™t\": \"should not\",\n",
    "    \"shouldnâ€™tâ€™ve\": \"should not have\",\n",
    "    \"soâ€™ve\": \"so have\",\n",
    "    \"thatâ€™d\": \"that would\",\n",
    "    \"thatâ€™dâ€™ve\": \"that would have\",\n",
    "    \"thereâ€™d\": \"there would\",\n",
    "    \"thereâ€™dâ€™ve\": \"there would have\",\n",
    "    \"theyâ€™d\": \"they would\",\n",
    "    \"theyâ€™dâ€™ve\": \"they would have\",\n",
    "    \"theyâ€™ll\": \"they will\",\n",
    "    \"theyâ€™llâ€™ve\": \"they will have\",\n",
    "    \"theyâ€™re\": \"they are\",\n",
    "    \"theyâ€™ve\": \"they have\",\n",
    "    \"toâ€™ve\": \"to have\",\n",
    "    \"wasnâ€™t\": \"was not\",\n",
    "    \"weâ€™d\": \"we would\",\n",
    "    \"weâ€™dâ€™ve\": \"we would have\",\n",
    "    \"weâ€™ll\": \"we will\",\n",
    "    \"weâ€™llâ€™ve\": \"we will have\",\n",
    "    \"weâ€™re\": \"we are\",\n",
    "    \"weâ€™ve\": \"we have\",\n",
    "    \"werenâ€™t\": \"were not\",\n",
    "    \"whatâ€™ll\": \"what will\",\n",
    "    \"whatâ€™llâ€™ve\": \"what will have\",\n",
    "    \"whatâ€™re\": \"what are\",\n",
    "    \"whatâ€™ve\": \"what have\",\n",
    "    \"whenâ€™ve\": \"when have\",\n",
    "    \"whereâ€™d\": \"where did\",\n",
    "    \"whereâ€™ve\": \"where have\",\n",
    "    \"whoâ€™ll\": \"who will\",\n",
    "    \"whoâ€™llâ€™ve\": \"who will have\",\n",
    "    \"whoâ€™ve\": \"who have\",\n",
    "    \"whyâ€™ve\": \"why have\",\n",
    "    \"willâ€™ve\": \"will have\",\n",
    "    \"wonâ€™t\": \"will not\",\n",
    "    \"wonâ€™tâ€™ve\": \"will not have\",\n",
    "    \"wouldâ€™ve\": \"would have\",\n",
    "    \"wouldnâ€™t\": \"would not\",\n",
    "    \"wouldnâ€™tâ€™ve\": \"would not have\",\n",
    "    \"yâ€™all\": \"you all\",\n",
    "    \"yâ€™allâ€™d\": \"you all would\",\n",
    "    \"yâ€™allâ€™dâ€™ve\": \"you all would have\",\n",
    "    \"yâ€™allâ€™re\": \"you all are\",\n",
    "    \"yâ€™allâ€™ve\": \"you all have\",\n",
    "    \"youâ€™d\": \"you would\",\n",
    "    \"youâ€™dâ€™ve\": \"you would have\",\n",
    "    \"youâ€™ll\": \"you will\",\n",
    "    \"youâ€™llâ€™ve\": \"you will have\",\n",
    "    \"youâ€™re\": \"you are\",\n",
    "    \"youâ€™ve\": \"you have\"\n",
    "}\n",
    "\n",
    "# Load English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def normalize_and_stem_tokens(tokens, lexicon):\n",
    "    stemmer = PorterStemmer()  # Initialize PorterStemmer\n",
    "    \n",
    "    # Filter out 'URL' tokens and tokens containing 'hash_' or 'punc_'\n",
    "    tokens = [\n",
    "        token for token in tokens \n",
    "        if token != \"URL\" and \"HASH_\" not in token and \"PUNC_\" not in token and \"EMOT_\" not in token\n",
    "    ]\n",
    "    \n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()  # Convert to lowercase\n",
    "        token = re.sub(r'[^\\w]', '', token)  # Remove non-word characters\n",
    "        normalized_token = lexicon.get(token, token)  # Normalize using lexicon\n",
    "        \n",
    "        # Check for non-empty tokens and if it's not a stop word\n",
    "        if normalized_token and normalized_token not in stop_words:\n",
    "            stemmed_token = stemmer.stem(normalized_token)  # Apply stemming\n",
    "            normalized_tokens.append(stemmed_token)\n",
    "    \n",
    "    # Remove empty tokens from the final list\n",
    "    normalized_tokens = [token for token in normalized_tokens if token]\n",
    "    \n",
    "    return normalized_tokens\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Process each tweet\n",
    "normalized_tokens_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tokens = row['token']  # Get the tokens from the 'token' column\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = eval(tokens)  # Convert string representation of list back to list\n",
    "\n",
    "    # Normalize and stem tokens\n",
    "    normalized_tokens = normalize_and_stem_tokens(tokens, normalization_lexicon)\n",
    "    \n",
    "    # Store the normalized tokens\n",
    "    normalized_tokens_list.append(normalized_tokens)\n",
    "\n",
    "# Add the normalized tokens as a new column in the DataFrame\n",
    "df['normalization'] = normalized_tokens_list\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Normalization completed and saved to '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68eb203-8f3b-42a7-9dcb-e4e787781472",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b558d-c93a-4f93-b9c2-d735d9c54af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'your own'\n",
    "consumer_secret = 'your own'\n",
    "access_token = 'your own'\n",
    "access_secret = 'your own'\n",
    "bearer_token = 'your own'\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "endpoint_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "rules = [\n",
    "    {\"value\": '(\"Election2024\" OR \"USElection2024\") -is:retweet lang:en', \"tag\": \"Election2024\"},\n",
    "    {\"value\": '(\"Trump\" OR \"Trump2024\") -is:retweet lang:en', \"tag\": \"Trump2024\"},\n",
    "    {\"value\": '(\"Harris\" OR \"Kamala Harris\") -is:retweet lang:en', \"tag\": \"KamalaHarris\"}\n",
    "]\n",
    "\n",
    "query_parameters = {\n",
    "    \"tweet.fields\": \"id,text,author_id,created_at,public_metrics,referenced_tweets\",\n",
    "    \"user.fields\": \"id,name,username,created_at,description,location,verified,public_metrics\",\n",
    "    \"expansions\": \"author_id\",\n",
    "    \"max_results\": 100,\n",
    "}\n",
    "def request_headers(bearer_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Set up the request headers. \n",
    "    Returns a dictionary summarising the bearer token authentication details.\n",
    "    \"\"\"\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "headers = request_headers(bearer_token)\n",
    "\n",
    "def connect_to_endpoint(endpoint_url: str, headers: dict, parameters: dict) -> json:\n",
    "    \"\"\"\n",
    "    Connects to the endpoint and requests data.\n",
    "    Returns a json with Twitter data if a 200 status code is yielded.\n",
    "    Programme stops if there is a problem with the request and sleeps\n",
    "    if there is a temporary problem accessing the endpoint.\n",
    "    \"\"\"\n",
    "    response = requests.request(\n",
    "        \"GET\", url=endpoint_url, headers=headers, params=parameters\n",
    "    )\n",
    "    response_status_code = response.status_code\n",
    "    if response_status_code != 200:\n",
    "        if response_status_code >= 400 and response_status_code < 500:\n",
    "            raise Exception(\n",
    "                \"Cannot get data, the program will stop!\\nHTTP {}: {}\".format(\n",
    "                    response_status_code, response.text\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        sleep_seconds = random.randint(5, 60)\n",
    "        print(\n",
    "            \"Cannot get data, your program will sleep for {} seconds...\\nHTTP {}: {}\".format(\n",
    "                sleep_seconds, response_status_code, response.text\n",
    "            )\n",
    "        )\n",
    "        time.sleep(sleep_seconds)\n",
    "        return connect_to_endpoint(endpoint_url, headers, parameters)\n",
    "    return response.json()\n",
    "\n",
    "def process_twitter_data(\n",
    "    json_response: json,\n",
    "    query_tag: str,\n",
    "    tweets_data: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds new tweet/user information to the table of\n",
    "    tweets/users and saves dataframes as pickle files,\n",
    "    if data is avaiable.\n",
    "    \n",
    "    Returns the tweets and users updated dataframes.\n",
    "\"\"\"\n",
    "   \n",
    "    if \"data\" in json_response.keys():\n",
    "        #new = pd.DataFrame(json_response)\n",
    "        new = pd.json_normalize(json_response)\n",
    "        tweets_data = pd.concat([tweets_data, new])\n",
    "        tweets_data.to_pickle(\"tweets_\" + query_tag + \".pkl\")\n",
    "\n",
    "    return tweets_data\n",
    "\n",
    "# Initialize two empty DataFrames to store tweet and user data\n",
    "tweets_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over each rule, send requests, and retrieve tweet and user data\n",
    "for i in range(len(rules)):\n",
    "    query_parameters[\"query\"] = rules[i][\"value\"]  # Set the query string for the current rule\n",
    "    query_tag = rules[i][\"tag\"]  # Get the tag for the current rule for file naming\n",
    "\n",
    "    # Connect to the Twitter API and get the JSON response\n",
    "    json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "    \n",
    "    # Process the JSON response and update the tweet data DataFrame\n",
    "    tweets_data = process_twitter_data(\n",
    "        json_response, query_tag, tweets_data\n",
    "    )\n",
    "\n",
    "    time.sleep(5)  # Pause for 5 seconds to comply with API rate limits\n",
    "\n",
    "    # Check if there is a next page of data to retrieve\n",
    "    while \"next_token\" in json_response[\"meta\"]:\n",
    "        query_parameters[\"next_token\"] = json_response[\"meta\"][\"next_token\"]  # Get the next page token\n",
    "\n",
    "        # Connect to the Twitter API again to retrieve the next page of data\n",
    "        json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "        \n",
    "        # Process the JSON response and update the tweet and user data DataFrame\n",
    "        tweets_data = process_twitter_data(\n",
    "            json_response, query_tag, tweets_data\n",
    "        )\n",
    "\n",
    "        time.sleep(5)  # Pause for 5 seconds to comply with API rate limits\n",
    "\n",
    "# Save tweet and user data as a JSON file\n",
    "tweets_data.to_json(\"tweets_data.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Data has been successfully saved as a JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa29df7-67e9-4cbb-829b-adb377aedaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store all the 'data' sections\n",
    "data_list = []\n",
    "\n",
    "# Read the JSON file line by line\n",
    "with open('tweets_data.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.strip():  # Ensure the line is not empty\n",
    "            json_line = json.loads(line)\n",
    "            if 'data' in json_line:  # Check if 'data' is in the JSON\n",
    "                data_list.extend(json_line['data'])  # Add the 'data' section to the list\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df1 = pd.DataFrame(data_list)\n",
    "\n",
    "# Create a list to store flattened data\n",
    "flat_data_list = []\n",
    "\n",
    "# Process each tweet\n",
    "for index, tweet in df1.iterrows():  # Use iterrows() to iterate over rows\n",
    "    # Extract public_metrics and referenced_tweets\n",
    "    public_metrics = tweet.get('public_metrics', {})\n",
    "    referenced_tweets = tweet.get('referenced_tweets')\n",
    "    \n",
    "    if isinstance(referenced_tweets, list):  # If it is a list\n",
    "        referenced_tweet_types = [t.get('type') for t in referenced_tweets if t.get('type') is not None]\n",
    "    else:\n",
    "        referenced_tweet_types = []  # Return an empty list if None or other non-list value\n",
    "\n",
    "    # Construct a flattened dictionary\n",
    "    flat_data = {\n",
    "        \"edit_history_tweet_ids\": tweet.get('edit_history_tweet_ids'),\n",
    "        \"created_at\": tweet.get('created_at'),\n",
    "        \"id\": tweet.get('id'),\n",
    "        \"author_id\": tweet.get('author_id'),\n",
    "        \"text\": tweet.get('text'),\n",
    "        \"retweet_count\": public_metrics.get('retweet_count', 0),\n",
    "        \"reply_count\": public_metrics.get('reply_count', 0),\n",
    "        \"like_count\": public_metrics.get('like_count', 0),\n",
    "        \"quote_count\": public_metrics.get('quote_count', 0),\n",
    "        \"bookmark_count\": public_metrics.get('bookmark_count', 0),\n",
    "        \"impression_count\": public_metrics.get('impression_count', 0),\n",
    "        \"referenced_tweet_types\": referenced_tweet_types\n",
    "    }\n",
    "\n",
    "    # Add the flattened dictionary to the list\n",
    "    flat_data_list.append(flat_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(flat_data_list)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('output_tweets.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd9366-1f04-4b03-8bd9-0101c7c202db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store all user data\n",
    "data_list = []\n",
    "\n",
    "# Read the JSON file line by line\n",
    "with open('tweets_data.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.strip():  # Ensure the line is not empty\n",
    "            json_line = json.loads(line)\n",
    "            if 'includes.users' in json_line:  # Check if 'includes.users' is in the JSON\n",
    "                data_list.extend(json_line['includes.users'])  # Add the 'users' section to the list\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "user_data_list = pd.DataFrame(data_list)\n",
    "\n",
    "# Create a list to store flattened data\n",
    "flat_data_list = []\n",
    "\n",
    "# Process each user data\n",
    "for index, user in user_data_list.iterrows():  # Use iterrows() to iterate over rows\n",
    "    # Construct a flattened dictionary\n",
    "    flat_data = {\n",
    "        \"name\": user.get('name'),\n",
    "        \"username\": user.get('username'),\n",
    "        \"description\": user.get('description'),\n",
    "        \"followers_count\": user.get('public_metrics', {}).get('followers_count', 0),\n",
    "        \"following_count\": user.get('public_metrics', {}).get('following_count', 0),\n",
    "        \"tweet_count\": user.get('public_metrics', {}).get('tweet_count', 0),\n",
    "        \"listed_count\": user.get('public_metrics', {}).get('listed_count', 0),\n",
    "        \"like_count\": user.get('public_metrics', {}).get('like_count', 0),\n",
    "        \"verified\": user.get('verified', False),\n",
    "        \"created_at\": user.get('created_at'),\n",
    "        \"location\": user.get('location'),\n",
    "        \"id\": user.get('id'),\n",
    "        \"withheld\": user.get('withheld')\n",
    "    }\n",
    "\n",
    "    # Add the flattened dictionary to the list\n",
    "    flat_data_list.append(flat_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(flat_data_list)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('output_users.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273a303e-bdae-4eeb-a0e2-6b73cdf776b7",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Transformers(BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efbc8f2-2dd5-4055-8f38-2a4867af57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "import emoji\n",
    "\n",
    "# Torch ML libraries\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Misc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f150ea-8d2d-4ba3-bfa2-42702cf66007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set intial variables and constants\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Graph Designs\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "# Random seed for reproducibilty\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Set GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93576a-33e7-486b-b876-0d42c48a4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691877b-ea1e-4381-ade5-16d40b550b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file containing processed sampled tweets\n",
    "df = pd.read_csv('cleanprocessed_sampled_tweets.csv')\n",
    "\n",
    "# Convert text to lowercase and replace non-alphanumeric characters and the word 'url' with a space\n",
    "df[\"text\"] = df[\"text\"].str.lower().str.replace(\"([^0-9A-Za-z \\t])|\\burl\\b\", \" \", case=False, regex=True)\n",
    "\n",
    "# Drop duplicate entries based on the 'text' column\n",
    "df = df.drop_duplicates(\"text\")\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    t = row[\"text\"]  # Get the original text\n",
    "    cleaned_text = strip_emoji(t)  # Remove emojis\n",
    "    # Update the processed text back to the original 'text' column\n",
    "    df.at[index, \"text\"] = cleaned_text\n",
    "\n",
    "# Print the number of unique reviews\n",
    "print(df.shape[0])  # Print the number of unique tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fc928-30cc-42a4-a6ab-96dc8594a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd796aa-2a62-4e85-9024-e68ce8975c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Draw a count plot for the happiness scores\n",
    "sns.countplot(x=df.happiness, palette=HAPPY_COLORS_PALETTE)\n",
    "\n",
    "# Calculate parameters for the normal distribution\n",
    "mean = 4\n",
    "height = 200\n",
    "lower_bound = 0\n",
    "upper_bound = 8\n",
    "std = (upper_bound - mean) / 2.576  # Calculate standard deviation\n",
    "\n",
    "# Generate x values for the normal distribution\n",
    "x = np.linspace(mean - 4*std, mean + 4*std, 1000)\n",
    "\n",
    "# Calculate the y values for the normal distribution\n",
    "y = stats.norm.pdf(x, mean, std)\n",
    "\n",
    "# Adjust y values to reach the specified height\n",
    "y = y / np.max(y) * height\n",
    "\n",
    "# Plot the normal distribution curve\n",
    "plt.plot(x, y, color='red', label='Fitted Curve', lw=2)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the graph\n",
    "plt.xlabel('Sentiment score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b4458-c80e-4670-a7b6-086c5706de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Happiness to sentiment\n",
    "def to_sentiment(rating):\n",
    "    \n",
    "    rating = int(rating)\n",
    "    \n",
    "    # Convert to class\n",
    "    if rating <= 4:\n",
    "        return 0\n",
    "    elif rating == 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Apply to the dataset \n",
    "df['sentiment'] = df.happiness.apply(to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad008a98-a460-4783-96c3-5ae9faa3fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "ax = sns.countplot(x=df.sentiment, palette=HAPPY_COLORS_PALETTE)\n",
    "plt.xlabel('tweet sentiment')\n",
    "ax.set_xticklabels(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749db9e3-cd8e-488c-bf7c-3ca9ed7598e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "# Build a BERT based tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e31ffa-9084-43e2-8efe-7df9f02d8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the common BERT tokens\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id) # marker for ending of a sentence\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of each sentence, so BERT knows weâ€™re doing classification\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id) # special token for padding\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id) # tokens not found in training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa983c1-60b8-46ea-b175-4764e7976c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each tweet \n",
    "token_lens = []\n",
    "\n",
    "# Iterate through the content slide\n",
    "for txt in df.text:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170f202-6eeb-4170-ba73-c5ff05f40c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of tweet lengths \n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2bafd4-9053-4e35-8167-38d45947e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b51de7-d73a-410b-be0b-fed37495e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiTweetsDataset(Dataset):\n",
    "    # Constructor Function \n",
    "    def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "        self.tweets = tweets\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    # Length magic method\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # get item magic method\n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweets[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        # Encoded format to be returned \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'tweet_text': tweet,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a6d3d-bf4a-41cd-a901-8a2c83f20d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a 80% train data and 10% test and 10% validation data\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b82f61-1f9d-4884-bb88-b8e053d74942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataloader to release data in batches.\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = SentiTweetsDataset(\n",
    "        tweets=df.text.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len)\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0b627-7535-423a-8a9d-1e667ba24919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, test and val data loaders\n",
    "BATCH_SIZE = 32\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a73ac-fac7-4858-a268-d141cdefd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples \n",
    "data = next(iter(train_data_loader))\n",
    "print(data.keys())\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28c8d9-c623-44d6-af63-c219e69bc37a",
   "metadata": {},
   "source": [
    "## Sentiment Classification with BERT and Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f586d-44fe-4735-bb18-6e9fd7612911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the basic BERT model \n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a87568-2f18-41fa-a0bc-7b1e0e51a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Sentiment Classifier class \n",
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    # Constructor class \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)  # Load the pre-trained BERT model\n",
    "        self.drop = nn.Dropout(p=0.3)  # Dropout layer to prevent overfitting\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)  # Output layer\n",
    "\n",
    "    # Forward propagation method\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]  # Get the pooled output\n",
    "\n",
    "        # Debug: Print the type of pooled_output\n",
    "        # print(\"Type of pooled_output:\", type(pooled_output))\n",
    "        \n",
    "        # Add a dropout layer \n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)  # Return the output from the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6cb668-0b38-4d46-b69e-5353b0da1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice if output is tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69031150-3636-42db-a734-2af0ffda15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and move to classifier\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef4c70-0c67-4f5e-af10-9e6eff15014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden units\n",
    "print(bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804268c-bb38-4865-be23-776f470f329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations \n",
    "EPOCHS = 15\n",
    "\n",
    "# Optimizer Adam \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps)\n",
    "\n",
    "# Set the loss function \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa3142-496c-4b98-bc42-afa75ba4c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for a single training iteration\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Descent\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2df88-27cc-46ff-b185-366df9b1952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to evaluate model performance\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de4d73-8bff-4cf1-8388-0a5d53fcb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Write the training Loop and store the best training state.\n",
    "\n",
    "# Initialize history to store training and validation metrics\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Show details \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train))\n",
    "    \n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "    \n",
    "    # Get model performance (accuracy and loss)\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val))\n",
    "    \n",
    "    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n",
    "    print()\n",
    "    \n",
    "    history['train_acc'].append(train_acc.item())\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc.item())\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    '''# If we beat prev performance\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96c677-197f-44e0-af67-5caa8ef09d86",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15280b3-7021-4cda-8ce9-82820eff59b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "# Graph chars\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79847891-4f9f-478a-bea3-e46ba1196fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model EvaluationÂ¶\n",
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94ee9f-d7f2-4999-8025-502f8c961a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    tweet_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"tweet_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            # Get outouts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            tweet_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "    return tweet_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54665b5-98b7-4d20-91d7-24ea8eb0c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    test_data_loader\n",
    ")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff264bd-9027-4257-b5b4-12ab57c16f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976902d-5f65-4955-957a-ee9e7e0bfe4a",
   "metadata": {},
   "source": [
    "## Predicting on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c874200-efdb-478e-b98f-aa242f824850",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Elon supports Trump\"\n",
    "\n",
    "encoded_test = tokenizer.encode_plus(\n",
    "    test_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "input_ids = encoded_test['input_ids'].to(device)\n",
    "attention_mask = encoded_test['attention_mask'].to(device)\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    "\n",
    "print(f'Test text: {test_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f1bec-05d1-4120-9ee0-03833cd4b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data to be classified\n",
    "data = pd.read_csv('classified_data_bayes.csv')\n",
    "\n",
    "# Clean the 'text' column\n",
    "data[\"text\"] = data[\"text\"].str.lower().str.replace(r\"([^0-9A-Za-z \\t])|\\burl\\b\", \"\", case=False, regex=True)\n",
    "\n",
    "# Iterate over each row, removing emojis\n",
    "for index, row in data.iterrows():\n",
    "    t = row[\"text\"]  # Get the original text\n",
    "    cleaned_text = strip_emoji(t)  # Remove emojis\n",
    "    # Update the processed text back to the original 'text' column\n",
    "    data.at[index, \"text\"] = cleaned_text\n",
    "    \n",
    "# Define a function for classification\n",
    "def classify_text(text):\n",
    "    encoded_test = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=MAX_LEN,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_test['input_ids'].to(device)\n",
    "    attention_mask = encoded_test['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "    _, prediction = torch.max(output, dim=1)  # Get the predicted class\n",
    "    return class_names[prediction.item()]\n",
    "\n",
    "# Classify each row in the 'text' column and store results in the 'sentiment' column\n",
    "data['sentiment'] = data['text'].apply(classify_text)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "data.to_csv('classified_data_sentiment_.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Classification completed and saved as 'classified_sentiment_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31503419-ced0-46d9-9dad-c0befedfcd12",
   "metadata": {},
   "source": [
    "# Word shift Graph lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360061ad-912d-4faa-94e4-105f65c809aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections.abc import Mapping\n",
    "import shifterator as sh\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#sns.set(font_scale=1.5)\n",
    "#sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b4845-7923-4973-ab37-fc9abe499379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(txt:str):\n",
    "\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "def clean_text(txt:str) -> {}:\n",
    "    \"\"\"Removes punctuation, changes to lowercase, removes\n",
    "        stopwords, removes \"animal\" and \"crossing\", and\n",
    "        calculates word frequencies (as counts).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        A text string that you want to clean.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Words and frequency counts\n",
    "    \"\"\"\n",
    "    \n",
    "    tmp = [remove_punctuation(t) for t in txt]\n",
    "    tmp = [t.lower().split() for t in tmp]\n",
    "    \n",
    "    tmp = [[w for w in t if not w in stop_words]\n",
    "              for t in tmp]\n",
    "#     tmp = [[w for w in t if not w in ['animal', 'crossing']]\n",
    "#                      for t in tmp]\n",
    "    \n",
    "    tmp = list(itertools.chain(*tmp))\n",
    "    tmp = collections.Counter(tmp)\n",
    "        \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33fa30-6a93-4bbc-aac7-4999ed30f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('classified_data_sentiment_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf4246-ddbf-47c8-8ea9-a189a96a1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.drop_duplicates(\"text\").shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53e328-8d65-4b9b-b612-86013e10e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower().str.replace(\"([^0-9A-Za-z \\t])|\\burl\\b\", \"\", case=False, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb9dfb-05dc-4a1a-a927-9689c35f4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(\"text\")\n",
    "print(df.shape[0]) # 6 duplicate reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f41bdb-3924-4d36-8190-b09ee3d41d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b1703-4dcc-4264-bde3-04901914dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = df[df['sentiment'] == 'negative']\n",
    "df_pos = df[df['sentiment'] == 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca0348-d79c-47a0-ba98-31fb4c6df868",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()\n",
    "texts_neg = df_neg['text'].tolist()\n",
    "texts_pos = df_pos['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decbb71-2968-407e-9ff6-31b1af676a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['puncexcl', 'puncques', \"hashelection2024\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f858a-8c9c-4269-864f-724b69eb9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the review texts\n",
    "clean_texts = clean_text(texts)\n",
    "clean_texts_neg = clean_text(texts_neg)\n",
    "clean_texts_pos = clean_text(texts_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb35a3b-6a92-44e8-b5b0-b65d4f9b0bbc",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9289d3a-d944-4743-a653-79e165a2755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1 = clean_texts,\n",
    "                                      type2freq_2 = clean_texts_neg,\n",
    "                                      type2score_1 = 'labMT_English',\n",
    "                                      type2score_2 = 'labMT_English',\n",
    "                                      stop_lens=[(4,6)])\n",
    "sentiment_shift.get_shift_graph(detailed=True,\n",
    "                                top_n=30,\n",
    "                                system_names=['all tweets', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d5dc2-84df-49cc-9751-3e21708559b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1 = clean_texts_neg,\n",
    "                                      type2freq_2 = clean_texts_pos,\n",
    "                                      type2score_1 = 'labMT_English',\n",
    "                                      type2score_2 = 'labMT_English',\n",
    "                                      stop_lens=[(4,6)])\n",
    "sentiment_shift.get_shift_graph(detailed=True,\n",
    "                                system_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d9dda-4bb1-4544-b184-b4a153c6dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# political"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e850f2-d00f-4256-b5f3-d75387b3bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df[df['support'] == 'Trump']\n",
    "df_o = df[(df['support'] == 'Others') | (df['support'] == 'Harris')]\n",
    "texts = df['text'].tolist()\n",
    "texts_t = df_t['text'].tolist()\n",
    "texts_o = df_o['text'].tolist()\n",
    "# Clean up the review texts\n",
    "clean_texts_t = clean_text(texts_t)\n",
    "clean_texts_o = clean_text(texts_o)\n",
    "df['support'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56804a2-bd0a-4489-a6e3-4da6aacce57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1 = clean_texts,\n",
    "                                      type2freq_2 = clean_texts_o,\n",
    "                                      type2score_1 = 'labMT_English',\n",
    "                                      type2score_2 = 'labMT_English',\n",
    "                                      stop_lens=[(4,6)])\n",
    "sentiment_shift.get_shift_graph(detailed=1,\n",
    "                                top_n=30,\n",
    "                                system_names=['All text', 'Others'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912d30e-56dd-41f9-95c7-7b78477e5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1 = clean_texts,\n",
    "                                      type2freq_2 = clean_texts_t,\n",
    "                                      type2score_1 = 'labMT_English',\n",
    "                                      type2score_2 = 'labMT_English',\n",
    "                                      stop_lens=[(4,6)])\n",
    "sentiment_shift.get_shift_graph(detailed=1,\n",
    "                                top_n=30,\n",
    "                                system_names=['All text', 'Trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b7675-9edb-4c5d-8ac9-a31b8a276d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
